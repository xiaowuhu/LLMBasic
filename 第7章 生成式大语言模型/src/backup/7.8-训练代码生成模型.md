
https://huggingface.co/blog/codeparrot


## 7.4 训练代码生成模型

深度学习爱好者使用 Python 编程语言的居多，所以我们就训练一个简单的 Python 代码编程助手吧，给它起一个名字叫做 PYCPilot（python code pilot）。

### 7.4.1 下载数据文件

在 GitHub 上有很多公开的 Python 代码库，读者可以去自己下载，也可以直接使用 HuggingFace 整理好的关于数据科学编程的代码子集。以下代码可以把它下载到本地的磁盘中，以 arrow（一种独立于语言的列式内存格式，用于在CPU和GPU等现代硬件上进行高效的分析操作）格式保存：

```python
print("load train dataset...")
ds_train = load_dataset("huggingface-course/codeparrot-ds-train", split="train")
print("load valid dataset...")
ds_valid = load_dataset("huggingface-course/codeparrot-ds-valid", split="validation")
```

缺省的存储位置是：

```
C:\Users\{your_name}\.cache\huggingface\datasets\
    + huggingface-course___codeparrot-ds-train  # 训练集
    + huggingface-course___codeparrot-ds-valid  # 验证集
```

第一个是训练集，第二个是验证集，分成了很多子文件，一共占用 6G 多的磁盘空间。把二者组织成一个数据字典：

```python
raw_datasets = DatasetDict(
    {
        "train": ds_train  #.shuffle().select(range(5000)),
        "valid": ds_valid  #.shuffle().select(range(500))
    }
)
```

打印输出数据集的基本信息：

```
DatasetDict({
    train: Dataset({  # 训练集
        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],
        num_rows: 606720
    })
    valid: Dataset({  # 验证集
        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],
        num_rows: 3322
    })
})
```

可以看到训练集有 60 多万个样本文件，验证集有 3300 多个文件。每条记录有六个字段，随机取出一条记录观察，得到其中的五个字段（`content` 字段除外）的信息如下：
```
REPO_NAME: sserrot/champion_relationships
PATH: venv/Lib/site-packages/ipykernel/inprocess/tests/test_kernel.py
COPIES: 1
SIZE: 3634
LICENSE: mit
```

`content` 字段是一个完整的源代码文件中的内容，如下所示（有节略以节省篇幅）：

```python
# Copyright (c) IPython Development Team.
# Distributed under the terms of the Modified BSD License.

from __future__ import print_function

import sys
import unittest
...
def _init_asyncio_patch():
    ...
    if sys.platform.startswith("win") and sys.version_info >= (3, 8):
        import asyncio
        try:
            ...
        except ImportError:
            pass
            # not affected
        else:
            ...
```
详情在【代码：H7_4_1_Data_Download.py】中。

### 7.4.2 训练分词器

在这里读者可以下载 HuggingFace 为上述数据集训练好的分词器：

```python
tokenizer = AutoTokenizer.from_pretrained("huggingface-course/code-search-net-tokenizer")
```

但考虑到读者可能会有自己的数据集，所以我们从原始数据中训练自己的分词器。不同于第 21 章中从文本文件中训练分词器，这里的数据文件是 arrow 格式，所以需要从 `datasets` 对象中来训练分词器。为了避免把一个大数据集都加载到内存里，可以定义一个迭代器，每次只加载 10000 条数据：
```python
# 迭代器，每次只加载 10000 条数据到内存
def get_training_corpus():
    dataset = raw_datasets["train"]
    for start_idx in range(0, len(dataset), 10000):
        samples = dataset[start_idx : start_idx + 10000]
        yield samples["content"]
```

然后从预训练的 GPT-2 中加载一个分词器，继承它的基本参数，训练一个新的分词器：

```python
# 训练分词器
print("train tokenizer...")
corpus_iterator = get_training_corpus()
# 加载已有的分词器，以避免指定一些繁琐的参数细节
gpt2_tokenizer = AutoTokenizer.from_pretrained("gpt2")
my_tokenizer = gpt2_tokenizer.train_new_from_iterator(corpus_iterator, 52000)
my_tokenizer.save_pretrained("../model/ch7/pycpilot/tokenizer/") # 保存到磁盘
```
这一步需要大约 10 分钟。保存好的分词器文件在 `../model/ch7/pycpilot/tokenizer/` 目录下，共有五个文件：
```
merges.txt              # 合并词文件
special_tokens_map.json # 特殊 token
tokenizer.json          # 最大的文件，包含 merges.txt 和 vocab.json
tokenizer_config.json   # 配置文件
vocab.json              # 词表文件
```
针对同一段代码文本：
```python
def add_numbers(a, b):
    """Add the two numbers `a` and `b`."""
    return a + b
```
检查新旧两个分词器的效果：

```
GPT-2的分词结果:
['def', 'Ġadd', '_', 'n', 'umbers', '(', 'a', ',', 'Ġb', '):', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ"""', 'Add', 
 'Ġthe', 'Ġtwo', 'Ġnumbers', 'Ġ`', 'a', '`', 'Ġand', 'Ġ`', 'b', '`', '."', '""', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 
 'Ġreturn', 'Ġa', 'Ġ+', 'Ġb']
新训练的分词器的结果:
['def', 'Ġadd', '_', 'numbers', '(', 'a', ',', 'Ġb', '):', 'ĊĠĠĠ', 'Ġ"""', 'Add', 'Ġthe', 'Ġtwo', 
 'Ġnumbers', 'Ġ`', 'a', '`', 'Ġand', 'Ġ`', 'b', '`."""', 'ĊĠĠĠ', 'Ġreturn', 'Ġa', 'Ġ+', 'Ġb']
```
可以看到新分词器的分词结果要更合理一些。

详情在【代码：H7_4_2_Train_Tokenizer.py】中。

### 7.4.3 生成数据集

接下来要使用新的分词器把数据文件变成 GPT 模型可以认识的数据集，即把文本 token 变成 token id。这个转换需要一些时间，所以我们预先做好，以节省训练时的资源消耗。

首先加载数据 `raw_datasets`，然后使用我们刚刚训练好的分词器进行分词，随机取训练集的三个样本做分词：

```python
context_length = 1024  # 设置截断长度
outputs = tokenizer(
    raw_datasets["train"][0:3]["content"], # 取三个样本的 content 字段
    truncation=True, # 允许截断
    max_length=context_length,  # 截断长度
    return_overflowing_tokens=True,  # 返回全部分块
    return_length=True, # 返回分块长度
)
```
分词器不仅仅完成 token 形式转换任务，还把一长串 token id 按照指定的截断长度（1024）进行分块，结果如下：
```
Input IDs length:    6
Input chunk lengths: [1024, 1024, 476, 1024, 607, 615] # 三个样本共有 6 块
Chunk mapping:       [0,    0,    0,   1,    1,   2]   # 样本文件对应到块
```
这三个样本一共产生了 6 个序列块，每个样本的最后一块的长度一般不会恰巧是 1024。接下来使用 `map()` 方法把分词、分块结果保存到磁盘，这个方法针对一批数据调用上面的分词器函数，也有迭代器的作用：

```python
print("raw datasets mapping...")
tokenized_datasets = raw_datasets.map(
    tokenize_function,  # 指定分词功能函数
    batched=True,       # 批处理
    remove_columns=raw_datasets["train"].column_names # 删除无用字段
)
tokenized_datasets.set_format("torch") # 设置为 torch tensor 格式
tokenized_datasets.save_to_disk("../data/ch22/pycpilot/tokenized_datasets")
```
这一步需要较长的时间（大约 30 分钟），最后保存到磁盘中。到此为止我们已经准备好了大约 7GB 的数据，下面可以开始训练了。

详情在【代码：H7_4_3_Data_Builder.py】中。

### 7.4.4 训练模型

由于硬件调节限制，我们无法训练 GPT-3 模型，而从第 7.1 节中的讲解来看，GPT-3 与 GPT-2 的差别主要是注意力机制和参数规模上。稀疏注意力机制主要是为超大的模型所设计，以减小内存使用，并没有对注意力机制有所增强。从参数规模上来看，表 7.1.1 中的 GPT-3 Small 对于我们来说也能接受，只不过把滑动窗口大小从 2048 缩小到 1024。

所以，我们设置模型参数如下：

```python
batch_size, num_epochs = 4, 5
context_length = 1024  # 设置序列最大长度, 与数据处理时保持一致
n_ctx = 1024        # 上下文滑动窗口
n_head = 12         # 多头注意力头数
n_layer = 12        # 解码单元层数
n_embed = 768,      # 词向量维度
n_positions = 1024  # 位置编码维度
vocab_size = 52000  # 词表大小
optimizer = AdamW(model.parameters(), lr=5e-5) # 优化器
```

其它的细节与第 6 章中的一致。由于数据量太大（超过60万条样本），批大小为 4，所以一个 epoch 就需要迭代 15 万次，整个训练时间需要按天计算。在大模型训练中，一般是使用较小的学习率，以避免梯度更新太大而导致前功尽弃。

```python
# 加载分词器
tokenizer = AutoTokenizer.from_pretrained("../model/ch22/pycpilot/tokenizer/")
# 加载训练数据
train_dataloader = data_loader(batch_size)
# 加载模型
config = AutoConfig.from_pretrained(
    "gpt2",                     # 继承 GPT-2 模型参数
    vocab_size=len(tokenizer),  # 设置为分词器词表的大小
    n_ctx=context_length,       # 设置为数据集序列最大长度
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
)
model = GPT2LMHeadModel(config)  # 初始化模型
# 定义优化器
optimizer = AdamW(model.parameters(), lr=5e-5)
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler( # 余弦退火，预热1000步
    name="cosine", optimizer=optimizer, num_warmup_steps=1000,
    num_training_steps=num_training_steps
)
# 准备训练
model.to(device)
model.train()
...
```
详情在【代码：H7_4_4_Train_Model.py】中。

### 7.4.5 检验模型效果

由于训练时间太长，所以在每个 epoch 结束后都需要保存模型，我们可以在训练没有结束时就先用这些临时的模型参数进行测试。
