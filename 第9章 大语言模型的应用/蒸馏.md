
https://transformers.run/c4/c16_finetune_llms/#1633-%E6%A8%A1%E5%9E%8B%E8%92%B8%E9%A6%8F%E5%92%8C%E6%A8%A1%E5%9E%8B%E5%89%AA%E6%9E%9D


16.3.3 模型蒸馏和模型剪枝
除了模型量化之外，模型蒸馏和模型剪枝也是常用的模型压缩方法，它们通过直接精简模型结构以减少参数数量。

模型蒸馏

模型蒸馏（Model Distillation）的目标是将复杂模型（称为教师模型）包含的知识迁移到简单模型（称为学生模型）中。以分类问题 为例，模型蒸馏的核心思想是引入额外的损失函数（称为蒸馏损失函数），训练学生模型的输出尽可能接近教师模型的输出。在实际应用中，蒸馏损失函数通常与分类损失函数（交叉熵损失函数）联合用于训练学生模型。

传统的模型蒸馏方法包括两种：基于反馈的知识蒸馏方法和基于特征的知识蒸馏方法。如图 16-5 所示。

model_distillation

图 16-5 基于反馈的知识蒸馏和基于特征的知识蒸馏
基于反馈的知识蒸馏主要关注教师模型最后一层输出的 logits，这些 logits 经过 softmax 变换后作为学生模型的“软标签”进行学习，优化目标是让学生模型输出的 logits 去近似教师模型输出的 logits，从而学习到教师模型的特有知识；而基于特征的知识蒸馏则关注教师模型的中间层输出的激活值，并使用这些激活值作为监督信息训练学生模型。相较于最终的预测分布，中间层特征提供了更为丰富的模型信息，有助于在模型蒸馏过程中实现更为有效的知识迁移。然而，这种方法也存在一些技术难点，如消除架构不一致的影响、目标层自动化选择等。

面向大语言模型的知识蒸馏旨在将大语言模型（教师模型）包含的知识迁移到小模型（学生模型）中。根据大语言模型权重是否可以获得，可以分别使用白盒模型蒸馏方法和黑盒模型蒸馏方法。其中，白盒模型蒸馏方法可以获取模型的权重来指导学生模型，典型的方法为 MINILLM；而黑盒模型蒸馏方法无法获得模型权重，只能使用大语言模型的输出信息来训练小模型，目前主要关注于蒸馏大模型的关键 能力，如上下文学习能力、思维链推理能力以及指令遵循能力。

相关

以思维链推理能力的蒸馏为例，可以利用大语言模型生成输入样本的思维链推理过程，作为标签以外的额外补充信息来帮助引导小模型学习。这种方法通过引入大模型对于问题的求解思路来提供额外的监督信息，从而让学生模型同时学习预测标签以及大模型生成的推理过程解释。

模型剪枝

模型剪枝（Model Pruning）的目标是在尽可能不损失模型性能的情况下，努力消减模型的参数数量。

传统模型剪枝方法一般可以被分为结构化剪枝和非结构化剪枝。结构化剪枝（Structured Pruning）旨在去除对于性能影响较小的模型组件，可以删除神经元、通道甚至中间层。其核心思想是在尽量保持模型预测精度的条件下，去除那些对于结果影响不大的结构单元，如注意力机制中的注意力头、前馈层权重中的特定维度等。而非结构化剪枝（Unstructured Pruning） 则主要关注去除模型权重矩阵中不重要的数值（不修改模型结构）。一般来说，非结构化剪枝会创建一个包含 0/1 的掩码矩阵，并将其与原始的权重相乘，其中 0 所在位置的权重则不会在模型的计算中产生作用。当剪枝完成后，那些被剪枝掉的位置只会存储数值 0，从而节省存储空间。

延伸

在实际应用中，非结构化剪枝一般可以实现更高的剪枝比率，但是不会显著加速模型的计算过程，因为被掩码的部分可能仍然需要参与计算。而结构化剪枝通过去除结构单元，可以显著减少所需的矩阵乘法次数，实现模型的压缩和加速。

与传统的模型剪枝类似，大语言模型的剪枝方法分为结构化和非结构化剪枝两类。其中，非结构化剪枝一般容易获得更高的压缩率，典型工作包括 SparseGPT，其可以实现 60% 模型参数的剪枝并较好保持困惑度。大模型结构化剪枝的研究也取得了较好的模型压缩效果，例如 LLM-prune 在 LLaMA (7B) 上剪枝了 20% 的参数并保持 93.6% 的预测精度、Sheared LLaMA 则将 LLaMA-2 (7B) 剪枝到 2.7B 参数规模并保持 87.8% 的预测精度。